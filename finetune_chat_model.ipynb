{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Config, GPT2LMHeadModel,AutoModelForCausalLM,BitsAndBytesConfig,TrainingArguments,AutoTokenizer\n",
    "from peft import LoraConfig,get_peft_model\n",
    "# tokenizer=AutoTokenizer.from_pretrained('tirthadagr8/custom-mbart-large-50')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-1B')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['NEPTUNE_API_TOKEN'] = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI2ZGUwMDYyOC04NmE0LTQyM2UtOTVjNi0wZjQ3ZGU2ZjM4M2IifQ==\"\n",
    "os.environ['NEPTUNE_PROJECT'] = 'tirthadagr8/model-feed'\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config=GPT2Config(vocab_size=len(tokenizer),bos_token_id=tokenizer.bos_token_id,eos_token_id=tokenizer.eos_token_id,n_embd=384,n_layer=12,n_head=8)\n",
    "# no_of_parameters=config.vocab_size*config.n_embd+config.n_layer*config.n_embd+config.n_layer*(4*config.n_embd*config.n_embd+4*config.n_embd+2*config.n_embd*4*config.n_embd+9*config.n_embd)+2*config.n_embd\n",
    "# size_of_model=no_of_parameters/(1.6*100000000)\n",
    "# print(f'Number of parameters would be:{no_of_parameters} the size would be:{size_of_model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dtype=torch.float16\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-1B',quantization_config=bnb_config,device_map=\"auto\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/tirth/Desktop/english_corpus.txt','r') as f:\n",
    "    text=f.readline()\n",
    "chunk_size = 128  # You can adjust this value\n",
    "array_of_strings = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in sorted(list(set(text))):\n",
    "#     if c not in tokenizer.vocab:\n",
    "#         print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1c168df253410f8d429fed70f29031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/66779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b275355721416395e6dda97ba8c1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7420 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset,Dataset\n",
    "from transformers import LlamaTokenizer\n",
    "\n",
    "# Load text data\n",
    "dataset = Dataset.from_dict({'text':array_of_strings})\n",
    "split_dataset = dataset.train_test_split(test_size=0.1)  # 80% train, 20% test\n",
    "# Initialize the tokenizer\n",
    "# tokenizer = LlamaTokenizer.from_pretrained('huggingface/llama-tokenizer')  # or use a compatible LLaMA tokenizer\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    '''\n",
    "    *** this will be used in future to train it for conversation format training\n",
    "    messages = [{\"role\": \"user\", \"content\": \"What is the capital of France.\"}]\n",
    "    input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    '''\n",
    "    # return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
    "    texts=[]\n",
    "    for text in examples['text']:\n",
    "        texts.append(text+tokenizer.eos_token)\n",
    "    return {'text':texts}\n",
    "\n",
    "tokenized_dataset = split_dataset.map(tokenize_function, batched=True)\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "test_dataset = tokenized_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t to him in person. She trusted her friend, but so much could happen. She waited impatiently for word.Hopes and dreams were dash<|end_of_text|>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"outputs\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 2,\n",
    "    warmup_steps = 5,\n",
    "    # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "    max_steps = 60,\n",
    "    learning_rate = 2e-4,\n",
    "    logging_steps = 1,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = 3407,\n",
    "    report_to = \"none\", # Use this for WandB etc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tirth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, dataset_num_proc. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\Users\\tirth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tirth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:314: UserWarning: You passed a `dataset_num_proc` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tirth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee24e6927aaf4f7f8a8cb874e22a774e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/74199 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "# from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 128,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = training_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca99f9bcc434905800a37b48edbc4c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7574, 'grad_norm': 3.1946589946746826, 'learning_rate': 4e-05, 'epoch': 0.0}\n",
      "{'loss': 4.2136, 'grad_norm': 8.23609447479248, 'learning_rate': 8e-05, 'epoch': 0.0}\n",
      "{'loss': 4.4372, 'grad_norm': 3.507418155670166, 'learning_rate': 0.00012, 'epoch': 0.0}\n",
      "{'loss': 3.7351, 'grad_norm': 3.453597068786621, 'learning_rate': 0.00016, 'epoch': 0.0}\n",
      "{'loss': 4.1614, 'grad_norm': 3.5995664596557617, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 3.5713, 'grad_norm': 3.7305750846862793, 'learning_rate': 0.00019636363636363636, 'epoch': 0.0}\n",
      "{'loss': 4.2325, 'grad_norm': 3.490417957305908, 'learning_rate': 0.00019272727272727274, 'epoch': 0.0}\n",
      "{'loss': 3.9536, 'grad_norm': 4.045252323150635, 'learning_rate': 0.0001890909090909091, 'epoch': 0.0}\n",
      "{'loss': 4.4251, 'grad_norm': 5.286133766174316, 'learning_rate': 0.00018545454545454545, 'epoch': 0.0}\n",
      "{'loss': 3.5718, 'grad_norm': 4.190104007720947, 'learning_rate': 0.00018181818181818183, 'epoch': 0.0}\n",
      "{'loss': 3.7556, 'grad_norm': 4.394358158111572, 'learning_rate': 0.0001781818181818182, 'epoch': 0.0}\n",
      "{'loss': 3.8754, 'grad_norm': 4.484439373016357, 'learning_rate': 0.00017454545454545454, 'epoch': 0.0}\n",
      "{'loss': 3.6845, 'grad_norm': 4.404365539550781, 'learning_rate': 0.0001709090909090909, 'epoch': 0.0}\n",
      "{'loss': 3.4773, 'grad_norm': 4.822956085205078, 'learning_rate': 0.00016727272727272728, 'epoch': 0.0}\n",
      "{'loss': 3.8453, 'grad_norm': 6.37666130065918, 'learning_rate': 0.00016363636363636366, 'epoch': 0.0}\n",
      "{'loss': 3.4418, 'grad_norm': 5.141486644744873, 'learning_rate': 0.00016, 'epoch': 0.0}\n",
      "{'loss': 4.2526, 'grad_norm': 5.447482585906982, 'learning_rate': 0.00015636363636363637, 'epoch': 0.0}\n",
      "{'loss': 3.0708, 'grad_norm': 4.925096035003662, 'learning_rate': 0.00015272727272727275, 'epoch': 0.0}\n",
      "{'loss': 2.9981, 'grad_norm': 4.32192325592041, 'learning_rate': 0.0001490909090909091, 'epoch': 0.0}\n",
      "{'loss': 3.6966, 'grad_norm': 5.085453987121582, 'learning_rate': 0.00014545454545454546, 'epoch': 0.0}\n",
      "{'loss': 4.5093, 'grad_norm': 6.7651543617248535, 'learning_rate': 0.00014181818181818184, 'epoch': 0.0}\n",
      "{'loss': 3.752, 'grad_norm': 4.7774577140808105, 'learning_rate': 0.0001381818181818182, 'epoch': 0.0}\n",
      "{'loss': 3.2873, 'grad_norm': 5.215339660644531, 'learning_rate': 0.00013454545454545455, 'epoch': 0.0}\n",
      "{'loss': 3.4215, 'grad_norm': 5.212347984313965, 'learning_rate': 0.00013090909090909093, 'epoch': 0.0}\n",
      "{'loss': 3.7872, 'grad_norm': 5.543010711669922, 'learning_rate': 0.00012727272727272728, 'epoch': 0.0}\n",
      "{'loss': 3.9062, 'grad_norm': 5.078266143798828, 'learning_rate': 0.00012363636363636364, 'epoch': 0.0}\n",
      "{'loss': 4.7818, 'grad_norm': 6.800460338592529, 'learning_rate': 0.00012, 'epoch': 0.0}\n",
      "{'loss': 3.701, 'grad_norm': 5.361934661865234, 'learning_rate': 0.00011636363636363636, 'epoch': 0.0}\n",
      "{'loss': 4.0051, 'grad_norm': 6.28774356842041, 'learning_rate': 0.00011272727272727272, 'epoch': 0.0}\n",
      "{'loss': 3.4788, 'grad_norm': 6.042741298675537, 'learning_rate': 0.00010909090909090909, 'epoch': 0.0}\n",
      "{'loss': 3.1932, 'grad_norm': 4.799851417541504, 'learning_rate': 0.00010545454545454545, 'epoch': 0.0}\n",
      "{'loss': 3.6732, 'grad_norm': 5.9976911544799805, 'learning_rate': 0.00010181818181818181, 'epoch': 0.0}\n",
      "{'loss': 3.4591, 'grad_norm': 5.599961757659912, 'learning_rate': 9.818181818181818e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6921, 'grad_norm': 6.128819942474365, 'learning_rate': 9.454545454545455e-05, 'epoch': 0.0}\n",
      "{'loss': 3.788, 'grad_norm': 5.219702243804932, 'learning_rate': 9.090909090909092e-05, 'epoch': 0.0}\n",
      "{'loss': 3.4763, 'grad_norm': 5.676793575286865, 'learning_rate': 8.727272727272727e-05, 'epoch': 0.0}\n",
      "{'loss': 3.55, 'grad_norm': 5.0939836502075195, 'learning_rate': 8.363636363636364e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6192, 'grad_norm': 6.372316837310791, 'learning_rate': 8e-05, 'epoch': 0.0}\n",
      "{'loss': 3.5679, 'grad_norm': 6.4593353271484375, 'learning_rate': 7.636363636363637e-05, 'epoch': 0.0}\n",
      "{'loss': 3.8159, 'grad_norm': 5.225330352783203, 'learning_rate': 7.272727272727273e-05, 'epoch': 0.0}\n",
      "{'loss': 3.5239, 'grad_norm': 5.301013946533203, 'learning_rate': 6.90909090909091e-05, 'epoch': 0.0}\n",
      "{'loss': 3.2153, 'grad_norm': 4.592678070068359, 'learning_rate': 6.545454545454546e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6013, 'grad_norm': 6.14927864074707, 'learning_rate': 6.181818181818182e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7161, 'grad_norm': 7.766605854034424, 'learning_rate': 5.818181818181818e-05, 'epoch': 0.0}\n",
      "{'loss': 3.4631, 'grad_norm': 6.090049743652344, 'learning_rate': 5.4545454545454546e-05, 'epoch': 0.0}\n",
      "{'loss': 4.0209, 'grad_norm': 7.919611930847168, 'learning_rate': 5.090909090909091e-05, 'epoch': 0.0}\n",
      "{'loss': 2.9495, 'grad_norm': 4.812371730804443, 'learning_rate': 4.7272727272727275e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7493, 'grad_norm': 6.233229637145996, 'learning_rate': 4.3636363636363636e-05, 'epoch': 0.0}\n",
      "{'loss': 3.5942, 'grad_norm': 5.045269966125488, 'learning_rate': 4e-05, 'epoch': 0.0}\n",
      "{'loss': 3.4887, 'grad_norm': 4.791062355041504, 'learning_rate': 3.6363636363636364e-05, 'epoch': 0.0}\n",
      "{'loss': 3.3535, 'grad_norm': 4.437846660614014, 'learning_rate': 3.272727272727273e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6558, 'grad_norm': 6.108297824859619, 'learning_rate': 2.909090909090909e-05, 'epoch': 0.0}\n",
      "{'loss': 3.2224, 'grad_norm': 5.953855991363525, 'learning_rate': 2.5454545454545454e-05, 'epoch': 0.0}\n",
      "{'loss': 3.1383, 'grad_norm': 4.664297580718994, 'learning_rate': 2.1818181818181818e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7459, 'grad_norm': 6.828664779663086, 'learning_rate': 1.8181818181818182e-05, 'epoch': 0.0}\n",
      "{'loss': 3.4732, 'grad_norm': 5.480804920196533, 'learning_rate': 1.4545454545454545e-05, 'epoch': 0.0}\n",
      "{'loss': 4.0916, 'grad_norm': 5.478170394897461, 'learning_rate': 1.0909090909090909e-05, 'epoch': 0.0}\n",
      "{'loss': 3.0728, 'grad_norm': 4.537382125854492, 'learning_rate': 7.272727272727272e-06, 'epoch': 0.0}\n",
      "{'loss': 3.7701, 'grad_norm': 6.221881866455078, 'learning_rate': 3.636363636363636e-06, 'epoch': 0.0}\n",
      "{'loss': 2.9956, 'grad_norm': 5.635647296905518, 'learning_rate': 0.0, 'epoch': 0.0}\n",
      "{'train_runtime': 50.1597, 'train_samples_per_second': 2.392, 'train_steps_per_second': 1.196, 'train_loss': 3.6743915915489196, 'epoch': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60, training_loss=3.6743915915489196, metrics={'train_runtime': 50.1597, 'train_samples_per_second': 2.392, 'train_steps_per_second': 1.196, 'total_flos': 21050801012736.0, 'train_loss': 3.6743915915489196, 'epoch': 0.0016172724699793797})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "msg='hi! how are you?'\n",
    "inputs = tokenizer(msg, return_tensors='pt', padding=True, \n",
    "                   truncation=True, max_length=128).to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_length=100, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<|begin_of_text|>hi! how are you? it's been a long day and I'm about to fall asleep. I know I should be working, but I'm tired and want to go to bed. I know that I should be doing something productive, but I'm just too tired to even think about it. I'm going to bed and I'm going to sleep and I'm going to wake up tomorrow.<|end_of_text|>\"]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>hi! how are you?']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
