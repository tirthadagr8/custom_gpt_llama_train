{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Config, GPT2LMHeadModel,AutoModelForCausalLM,BitsAndBytesConfig,TrainingArguments,AutoTokenizer\n",
    "from peft import LoraConfig,get_peft_model\n",
    "from datasets import load_dataset,Dataset,load_dataset\n",
    "# tokenizer=AutoTokenizer.from_pretrained('tirthadagr8/custom-mbart-large-50')\n",
    "from trl import setup_chat_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-1B')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['NEPTUNE_API_TOKEN'] = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI2ZGUwMDYyOC04NmE0LTQyM2UtOTVjNi0wZjQ3ZGU2ZjM4M2IifQ==\"\n",
    "os.environ['NEPTUNE_PROJECT'] = 'tirthadagr8/model-feed'\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config=GPT2Config(vocab_size=len(tokenizer),bos_token_id=tokenizer.bos_token_id,eos_token_id=tokenizer.eos_token_id,n_embd=384,n_layer=12,n_head=8)\n",
    "# no_of_parameters=config.vocab_size*config.n_embd+config.n_layer*config.n_embd+config.n_layer*(4*config.n_embd*config.n_embd+4*config.n_embd+2*config.n_embd*4*config.n_embd+9*config.n_embd)+2*config.n_embd\n",
    "# size_of_model=no_of_parameters/(1.6*100000000)\n",
    "# print(f'Number of parameters would be:{no_of_parameters} the size would be:{size_of_model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dtype=torch.float16\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-1B',quantization_config=bnb_config,device_map=\"auto\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "model,tokenizer=setup_chat_format(model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('ruslanmv/ai-medical-chatbot', split=\"all\")\n",
    "split_dataset = dataset.train_test_split(test_size=0.1)  # 80% train, 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb9b15af412742e98e26bfe262ff0ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/231224 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82ea2f29485492e93e79251c7a7aa76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25692 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    '''\n",
    "    *** this will be used in future to train it for conversation format training\n",
    "    messages = [{\"role\": \"user\", \"content\": \"What is the capital of France.\"}]\n",
    "    input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    *** you can make your own custom template as well, like alpaca prompt below:-\n",
    "    alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction:\n",
    "    {}\n",
    "\n",
    "    ### Input:\n",
    "    {}\n",
    "\n",
    "    ### Response:\n",
    "    {}\"\"\"\n",
    "    '''\n",
    "    messages = [{\"role\": \"user\", \"content\": examples['Patient']},\n",
    "                {\"role\": \"assistant\", \"content\": examples['Doctor']}]\n",
    "    return {'text':tokenizer.apply_chat_template(messages,tokenize=False)}\n",
    "\n",
    "tokenized_dataset = split_dataset.map(tokenize_function)\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "test_dataset = tokenized_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nSymptoms are lower back pain - sometimes radiates down left leg Blood pressure usually run low - it has been running higher Urine - may have a little foul smell - Have had headaches a few days - but not today Recently protein was found in urine - last urine test it was clear<|im_end|>\\n<|im_start|>assistant\\nHello and Welcome to â€˜Ask A Doctorâ€™ service. I have reviewed your query and here is my advice. I have gone through the details and in my opinion you have kidney disease causing proteins in the urine, Backache and mild fever. Even blood pressure variations are known in kidney disease. Please get urine examination including culture and ultrasound abdomen. Till the diagnosis is clear take only acetaminophen ,when there is pain. Hope I have answered your query. Let me know if I can assist you further.<|im_end|>\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tirth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir = \"outputs\",\n",
    "    overwrite_output_dir=True,\n",
    "    max_seq_length = 128,\n",
    "    dataset_num_proc = 2,\n",
    "    dataset_text_field = \"text\",\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 2,\n",
    "    warmup_steps = 5,\n",
    "    # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "    max_steps = 60,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    learning_rate = 2e-4,\n",
    "    logging_steps = 1,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    seed = 3407,\n",
    "    report_to = \"none\", # Use this for WandB etc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fab58fa4afc46d0af2d845d29fae205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/231224 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d4c376f8834e608b04ddb4711caf89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/25692 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer,SFTConfig\n",
    "from transformers import TrainingArguments,Trainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = training_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6cff2eb232a48d0bd911d965e94c13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6236, 'grad_norm': 4.508493900299072, 'learning_rate': 4e-05, 'epoch': 0.0}\n",
      "{'loss': 3.0699, 'grad_norm': 2.656003475189209, 'learning_rate': 8e-05, 'epoch': 0.0}\n",
      "{'loss': 3.5535, 'grad_norm': 4.132633209228516, 'learning_rate': 0.00012, 'epoch': 0.0}\n",
      "{'loss': 3.5903, 'grad_norm': 3.653149127960205, 'learning_rate': 0.00016, 'epoch': 0.0}\n",
      "{'loss': 3.3466, 'grad_norm': 3.6871070861816406, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 3.2498, 'grad_norm': 4.367644309997559, 'learning_rate': 0.00019636363636363636, 'epoch': 0.0}\n",
      "{'loss': 3.3682, 'grad_norm': 4.806777000427246, 'learning_rate': 0.00019272727272727274, 'epoch': 0.0}\n",
      "{'loss': 3.7933, 'grad_norm': 4.472368240356445, 'learning_rate': 0.0001890909090909091, 'epoch': 0.0}\n",
      "{'loss': 3.2014, 'grad_norm': 5.618427753448486, 'learning_rate': 0.00018545454545454545, 'epoch': 0.0}\n",
      "{'loss': 3.8976, 'grad_norm': 3.2326624393463135, 'learning_rate': 0.00018181818181818183, 'epoch': 0.0}\n",
      "{'loss': 3.2003, 'grad_norm': 2.7177734375, 'learning_rate': 0.0001781818181818182, 'epoch': 0.0}\n",
      "{'loss': 3.2267, 'grad_norm': 3.2928693294525146, 'learning_rate': 0.00017454545454545454, 'epoch': 0.0}\n",
      "{'loss': 3.7906, 'grad_norm': 44.63957214355469, 'learning_rate': 0.0001709090909090909, 'epoch': 0.0}\n",
      "{'loss': 3.0491, 'grad_norm': 5.237500190734863, 'learning_rate': 0.00016727272727272728, 'epoch': 0.0}\n",
      "{'loss': 3.5489, 'grad_norm': 3.651111125946045, 'learning_rate': 0.00016363636363636366, 'epoch': 0.0}\n",
      "{'loss': 3.2804, 'grad_norm': 3.2680935859680176, 'learning_rate': 0.00016, 'epoch': 0.0}\n",
      "{'loss': 3.1954, 'grad_norm': 3.166640043258667, 'learning_rate': 0.00015636363636363637, 'epoch': 0.0}\n",
      "{'loss': 3.4834, 'grad_norm': 4.465459823608398, 'learning_rate': 0.00015272727272727275, 'epoch': 0.0}\n",
      "{'loss': 3.1017, 'grad_norm': 2.4206604957580566, 'learning_rate': 0.0001490909090909091, 'epoch': 0.0}\n",
      "{'loss': 2.9183, 'grad_norm': 2.4668071269989014, 'learning_rate': 0.00014545454545454546, 'epoch': 0.0}\n",
      "{'loss': 3.01, 'grad_norm': 2.349567174911499, 'learning_rate': 0.00014181818181818184, 'epoch': 0.0}\n",
      "{'loss': 3.5184, 'grad_norm': 2.800020456314087, 'learning_rate': 0.0001381818181818182, 'epoch': 0.0}\n",
      "{'loss': 3.7111, 'grad_norm': 2.971035957336426, 'learning_rate': 0.00013454545454545455, 'epoch': 0.0}\n",
      "{'loss': 3.6615, 'grad_norm': 3.139801502227783, 'learning_rate': 0.00013090909090909093, 'epoch': 0.0}\n",
      "{'loss': 3.2466, 'grad_norm': 3.752614974975586, 'learning_rate': 0.00012727272727272728, 'epoch': 0.0}\n",
      "{'loss': 3.4643, 'grad_norm': 2.942728042602539, 'learning_rate': 0.00012363636363636364, 'epoch': 0.0}\n",
      "{'loss': 3.4611, 'grad_norm': 3.011631965637207, 'learning_rate': 0.00012, 'epoch': 0.0}\n",
      "{'loss': 3.1124, 'grad_norm': 2.356299638748169, 'learning_rate': 0.00011636363636363636, 'epoch': 0.0}\n",
      "{'loss': 2.9828, 'grad_norm': 2.058513641357422, 'learning_rate': 0.00011272727272727272, 'epoch': 0.0}\n",
      "{'loss': 3.5619, 'grad_norm': 2.523106336593628, 'learning_rate': 0.00010909090909090909, 'epoch': 0.0}\n",
      "{'loss': 3.3237, 'grad_norm': 2.3150055408477783, 'learning_rate': 0.00010545454545454545, 'epoch': 0.0}\n",
      "{'loss': 2.6491, 'grad_norm': 2.2146799564361572, 'learning_rate': 0.00010181818181818181, 'epoch': 0.0}\n",
      "{'loss': 3.4921, 'grad_norm': 3.2235257625579834, 'learning_rate': 9.818181818181818e-05, 'epoch': 0.0}\n",
      "{'loss': 3.2337, 'grad_norm': 2.631263494491577, 'learning_rate': 9.454545454545455e-05, 'epoch': 0.0}\n",
      "{'loss': 3.096, 'grad_norm': 2.3547074794769287, 'learning_rate': 9.090909090909092e-05, 'epoch': 0.0}\n",
      "{'loss': 3.1704, 'grad_norm': 2.3406684398651123, 'learning_rate': 8.727272727272727e-05, 'epoch': 0.0}\n",
      "{'loss': 3.3316, 'grad_norm': 2.6303727626800537, 'learning_rate': 8.363636363636364e-05, 'epoch': 0.0}\n",
      "{'loss': 2.7548, 'grad_norm': 2.5220601558685303, 'learning_rate': 8e-05, 'epoch': 0.0}\n",
      "{'loss': 3.4765, 'grad_norm': 3.3546485900878906, 'learning_rate': 7.636363636363637e-05, 'epoch': 0.0}\n",
      "{'loss': 3.1407, 'grad_norm': 2.5822184085845947, 'learning_rate': 7.272727272727273e-05, 'epoch': 0.0}\n",
      "{'loss': 3.0352, 'grad_norm': 2.2538578510284424, 'learning_rate': 6.90909090909091e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7754, 'grad_norm': 3.556248426437378, 'learning_rate': 6.545454545454546e-05, 'epoch': 0.0}\n",
      "{'loss': 2.9583, 'grad_norm': 2.2514564990997314, 'learning_rate': 6.181818181818182e-05, 'epoch': 0.0}\n",
      "{'loss': 3.3506, 'grad_norm': 2.8046116828918457, 'learning_rate': 5.818181818181818e-05, 'epoch': 0.0}\n",
      "{'loss': 2.8132, 'grad_norm': 2.248382329940796, 'learning_rate': 5.4545454545454546e-05, 'epoch': 0.0}\n",
      "{'loss': 3.0979, 'grad_norm': 2.130335569381714, 'learning_rate': 5.090909090909091e-05, 'epoch': 0.0}\n",
      "{'loss': 3.3078, 'grad_norm': 2.4958913326263428, 'learning_rate': 4.7272727272727275e-05, 'epoch': 0.0}\n",
      "{'loss': 3.2416, 'grad_norm': 2.512190103530884, 'learning_rate': 4.3636363636363636e-05, 'epoch': 0.0}\n",
      "{'loss': 2.7781, 'grad_norm': 2.712122917175293, 'learning_rate': 4e-05, 'epoch': 0.0}\n",
      "{'loss': 3.4278, 'grad_norm': 2.897465229034424, 'learning_rate': 3.6363636363636364e-05, 'epoch': 0.0}\n",
      "{'loss': 2.8302, 'grad_norm': 2.387732982635498, 'learning_rate': 3.272727272727273e-05, 'epoch': 0.0}\n",
      "{'loss': 2.6883, 'grad_norm': 2.4107825756073, 'learning_rate': 2.909090909090909e-05, 'epoch': 0.0}\n",
      "{'loss': 3.0716, 'grad_norm': 2.726003885269165, 'learning_rate': 2.5454545454545454e-05, 'epoch': 0.0}\n",
      "{'loss': 3.2321, 'grad_norm': 2.3155126571655273, 'learning_rate': 2.1818181818181818e-05, 'epoch': 0.0}\n",
      "{'loss': 2.9738, 'grad_norm': 2.823356866836548, 'learning_rate': 1.8181818181818182e-05, 'epoch': 0.0}\n",
      "{'loss': 3.0841, 'grad_norm': 2.6649768352508545, 'learning_rate': 1.4545454545454545e-05, 'epoch': 0.0}\n",
      "{'loss': 3.1449, 'grad_norm': 2.4525179862976074, 'learning_rate': 1.0909090909090909e-05, 'epoch': 0.0}\n",
      "{'loss': 3.0971, 'grad_norm': 2.9567739963531494, 'learning_rate': 7.272727272727272e-06, 'epoch': 0.0}\n",
      "{'loss': 2.7694, 'grad_norm': 2.3164262771606445, 'learning_rate': 3.636363636363636e-06, 'epoch': 0.0}\n",
      "{'loss': 3.136, 'grad_norm': 2.6436610221862793, 'learning_rate': 0.0, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tirth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\utils\\save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 51.3826, 'train_samples_per_second': 2.335, 'train_steps_per_second': 1.168, 'train_loss': 3.245014444986979, 'epoch': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60, training_loss=3.245014444986979, metrics={'train_runtime': 51.3826, 'train_samples_per_second': 2.335, 'train_steps_per_second': 1.168, 'total_flos': 89731697246208.0, 'train_loss': 3.245014444986979, 'epoch': 0.0005189772687956267})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hi.  Anxiety is a very common condition and it is very important to identify the cause and treat it.  There are many types of anxiety and all of them have the common symptoms.  Anxiety is a feeling of unease, nervousness, restlessness, fear, worry, apprehension, apprehension, nervousness, restlessness, fear, worry, apprehension, apprehension, apprehension, apprehension, apprehension, apprehension, apprehension, apprehension, apprehension, apprehension, apprehension, apprehension, apprehension, apprehension, apprehension, apprehension, apprehension\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I often feel anxious in social situations. What are some ways to manage anxiety without medication?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, \n",
    "                                       add_generation_prompt=True)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, \n",
    "                   truncation=True, max_length=512).to(\"cuda\")\n",
    "\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=150, num_return_sequences=1)#, num_beams=10, early_stopping=False, repetition_penalty=2.2)\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(text.split(\"assistant\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg='hi! how are you?'\n",
    "inputs = tokenizer(msg, return_tensors='pt', padding=True, \n",
    "                   truncation=True, max_length=128).to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_length=100, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>hi! how are you? i have a problem, i have a 2003 honda accord, and it has a problem, it starts to smoke, it is a black smoke, it is a normal smoke, but the smoke is not normal, it is like a white smoke, like a white cloud, like a smoke, like a smoke, like a smoke, like a smoke, like a smoke, like a smoke, like a smoke, like a smoke, like a smoke, like']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>hi! how are you?']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
