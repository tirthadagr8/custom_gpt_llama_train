{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ’§ LFM2 - SFT with TRL\n",
        "\n",
        "This tutorial demonstrates how to fine-tune our LFM2 models, e.g. [`LiquidAI/LFM2-1.2B`](), using the TRL library.\n",
        "\n",
        "Follow along if it's your first time using trl, or take single code snippets for your own workflow\n",
        "\n",
        "## ðŸŽ¯ What You'll Find:\n",
        "- **SFT** (Supervised Fine-Tuning) - Basic instruction following\n",
        "- **LoRA + SFT** - Using LoRA (from PEFT) to SFT while on constrained hardware\n",
        "\n",
        "## ðŸ“‹ Prerequisites:\n",
        "- **GPU Runtime**: Select GPU in `Runtime` â†’ `Change runtime type`\n",
        "- **Hugging Face Account**: For accessing models and datasets\n",
        "\n"
      ],
      "metadata": {
        "id": "a3PTFH-H9Ozk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“¦ Installation & Setup\n",
        "\n",
        "First, let's install all the required packages:\n"
      ],
      "metadata": {
        "id": "x0RPLu2h9ome"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git trl>=0.18.2 peft>=0.15.2"
      ],
      "metadata": {
        "id": "3FIcp_wo9nsR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a842f19-fa43-4c4d-f44c-4a4bcbffccda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-09stnrv0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now verify the packages are installed correctly"
      ],
      "metadata": {
        "id": "41UEf1uxCd6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "import trl\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "print(f\"ðŸ“¦ PyTorch version: {torch.__version__}\")\n",
        "print(f\"ðŸ¤— Transformers version: {transformers.__version__}\")\n",
        "print(f\"ðŸ“Š TRL version: {trl.__version__}\")"
      ],
      "metadata": {
        "id": "bSJgYtHT_Os4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the model from Transformers ðŸ¤—\n",
        "\n"
      ],
      "metadata": {
        "id": "v_uXLzxQ_rnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from IPython.display import display, HTML, Markdown\n",
        "import torch\n",
        "\n",
        "model_id = \"LiquidAI/LFM2-1.2B\" # <- or LFM2-700M or LFM2-350M\n",
        "\n",
        "print(\"ðŸ“š Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "print(\"ðŸ§  Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"bfloat16\",\n",
        "#   attn_implementation=\"flash_attention_2\" <- uncomment on compatible GPU\n",
        ")\n",
        "\n",
        "print(\"âœ… Local model loaded successfully!\")\n",
        "print(f\"ðŸ”¢ Parameters: {model.num_parameters():,}\")\n",
        "print(f\"ðŸ“– Vocab size: {len(tokenizer)}\")\n",
        "print(f\"ðŸ’¾ Model size: ~{model.num_parameters() * 2 / 1e9:.1f} GB (bfloat16)\")"
      ],
      "metadata": {
        "id": "iA3erKM4-HhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŽ¯ Part 1: Supervised Fine-Tuning (SFT)\n",
        "\n",
        "SFT teaches the model to follow instructions by training on input-output pairs (instruction vs response). This is the foundation for creating instruction-following models."
      ],
      "metadata": {
        "id": "6ABA6Yrm_lql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load an SFT Dataset\n",
        "\n",
        "We will use [HuggingFaceTB/smoltalk](https://huggingface.co/datasets/HuggingFaceTB/smoltalk), limiting ourselves to the first 5k samples for brevity. Feel free to change the limit by changing the slicing index in the parameter `split`."
      ],
      "metadata": {
        "id": "KufdgeypHtst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "print(\"ðŸ“¥ Loading SFT dataset...\")\n",
        "train_dataset_sft = load_dataset(\"HuggingFaceTB/smoltalk\", \"all\", split=\"train[:5000]\")\n",
        "eval_dataset_sft = load_dataset(\"HuggingFaceTB/smoltalk\", \"all\", split=\"test[:500]\")\n",
        "\n",
        "print(\"âœ… SFT Dataset loaded:\")\n",
        "print(f\"   ðŸ“š Train samples: {len(train_dataset_sft)}\")\n",
        "print(f\"   ðŸ§ª Eval samples: {len(eval_dataset_sft)}\")\n",
        "print(f\"\\nðŸ“ Single Sample: {train_dataset_sft[0]['messages']}\")"
      ],
      "metadata": {
        "id": "XCe8O06-_Cps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Launch Training\n",
        "\n",
        "We are now ready to launch an SFT run with `SFTTrainer`, feel free to modify `SFTConfig` to play around with different configurations.\n",
        "\n"
      ],
      "metadata": {
        "id": "n5pI5JWpIlFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    output_dir=\"./lfm2-sft\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    warmup_steps=100,\n",
        "    warmup_ratio=0.2,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=None,\n",
        "    bf16=False # <- not all colab GPUs support bf16\n",
        ")\n",
        "\n",
        "print(\"ðŸ—ï¸  Creating SFT trainer...\")\n",
        "sft_trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=sft_config,\n",
        "    train_dataset=train_dataset_sft,\n",
        "    eval_dataset=eval_dataset_sft,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"\\nðŸš€ Starting SFT training...\")\n",
        "sft_trainer.train()\n",
        "\n",
        "print(\"ðŸŽ‰ SFT training completed!\")\n",
        "\n",
        "sft_trainer.save_model()\n",
        "print(f\"ðŸ’¾ SFT model saved to: {sft_config.output_dir}\")"
      ],
      "metadata": {
        "id": "ixD8Po-eAbPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŽ›ï¸ Part 2: LoRA + SFT (Parameter-Efficient Fine-tuning)\n",
        "\n",
        "LoRA (Low-Rank Adaptation) allows efficient fine-tuning by only training a small number of additional parameters. Perfect for limited compute resources!\n"
      ],
      "metadata": {
        "id": "08Y3TxKrBRXo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrap the model with PEFT\n",
        "\n",
        "We specify target modules that will be finetuned while the rest of the models weights remains frozen. Feel free to modify the `r` (rank) value:\n",
        "- higher -> better approximation of full-finetuning\n",
        "- lower -> needs even less compute resources"
      ],
      "metadata": {
        "id": "-MfWfc-Pvl9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "GLU_MODULES = [\"w1\", \"w2\", \"w3\"]\n",
        "MHA_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]\n",
        "CONV_MODULES = [\"in_proj\", \"out_proj\"]\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=8,  # <- lower values = fewer parameters\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=GLU_MODULES + MHA_MODULES + CONV_MODULES,\n",
        "    bias=\"none\",\n",
        "    modules_to_save=None,\n",
        ")\n",
        "\n",
        "lora_model = get_peft_model(model, lora_config)\n",
        "lora_model.print_trainable_parameters()\n",
        "\n",
        "print(\"âœ… LoRA configuration applied!\")\n",
        "print(f\"ðŸŽ›ï¸  LoRA rank: {lora_config.r}\")\n",
        "print(f\"ðŸ“Š LoRA alpha: {lora_config.lora_alpha}\")\n",
        "print(f\"ðŸŽ¯ Target modules: {lora_config.target_modules}\")"
      ],
      "metadata": {
        "id": "puYp_gTpBSsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Launch Training\n",
        "\n",
        "Now ready to launch the SFT training, but this time with the LoRA-wrapped model"
      ],
      "metadata": {
        "id": "L1Hem_DOwHgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_sft_config = SFTConfig(\n",
        "    output_dir=\"./lfm2-sft-lora\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    warmup_steps=100,\n",
        "    warmup_ratio=0.2,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=None,\n",
        ")\n",
        "\n",
        "print(\"ðŸ—ï¸  Creating LoRA SFT trainer...\")\n",
        "lora_sft_trainer = SFTTrainer(\n",
        "    model=lora_model,\n",
        "    args=lora_sft_config,\n",
        "    train_dataset=train_dataset_sft,\n",
        "    eval_dataset=eval_dataset_sft,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"\\nðŸš€ Starting LoRA + SFT training...\")\n",
        "lora_sft_trainer.train()\n",
        "\n",
        "print(\"ðŸŽ‰ LoRA + SFT training completed!\")\n",
        "\n",
        "lora_sft_trainer.save_model()\n",
        "print(f\"ðŸ’¾ LoRA model saved to: {lora_sft_config.output_dir}\")\n"
      ],
      "metadata": {
        "id": "u-VYQysHBY8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save merged model\n",
        "\n",
        "Merge the extra weights learned with LoRA back into the model to obtain a \"normal\" model checkpoint."
      ],
      "metadata": {
        "id": "xI1-N-_Ev0cC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nðŸ”„ Merging LoRA weights...\")\n",
        "merged_model = lora_model.merge_and_unload()\n",
        "merged_model.save_pretrained(\"./lfm2-lora-merged\")\n",
        "tokenizer.save_pretrained(\"./lfm2-lora-merged\")\n",
        "print(\"ðŸ’¾ Merged model saved to: ./lfm2-lora-merged\")"
      ],
      "metadata": {
        "id": "_rizEFUsvwce"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}