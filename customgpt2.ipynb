{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9920640,"sourceType":"datasetVersion","datasetId":6097009}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n%pip install -U neptune\n# %pip install -U transformers","metadata":{"_uuid":"85596513-c49e-4382-b47e-c4dcdd7b45ca","_cell_guid":"6b0555db-bf27-45c8-bfa0-865cdee90692","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-16T08:35:50.158886Z","iopub.execute_input":"2024-11-16T08:35:50.159176Z","iopub.status.idle":"2024-11-16T08:36:13.436596Z","shell.execute_reply.started":"2024-11-16T08:35:50.159139Z","shell.execute_reply":"2024-11-16T08:36:13.435524Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, GPT2Config, GPT2Model, AutoConfig,\\\n        AutoModelForCausalLM, Seq2SeqTrainer, Seq2SeqTrainingArguments,Gemma2Config\nfrom datasets import load_dataset,Dataset\nimport numpy as np\nimport torch\nfrom transformers import DataCollatorWithPadding,Trainer, TrainingArguments, DataCollatorForLanguageModeling","metadata":{"execution":{"iopub.status.busy":"2024-11-16T08:43:39.264133Z","iopub.execute_input":"2024-11-16T08:43:39.264541Z","iopub.status.idle":"2024-11-16T08:43:58.330784Z","shell.execute_reply.started":"2024-11-16T08:43:39.264502Z","shell.execute_reply":"2024-11-16T08:43:58.329763Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nfrom kaggle_secrets import UserSecretsClient\n\nneptune_api = UserSecretsClient().get_secret(\"NEPTUNE_API_TOKEN\")\nneptune_project =UserSecretsClient().get_secret(\"NEPTUNE_PROJECT\")\nbase_model_id='tirthadagr8/Japanese_to_english_gpt2CasualLM_GemmaTokenizer'\n# notebook_login(hf_token)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T08:44:06.432388Z","iopub.execute_input":"2024-11-16T08:44:06.433471Z","iopub.status.idle":"2024-11-16T08:44:06.760799Z","shell.execute_reply.started":"2024-11-16T08:44:06.433427Z","shell.execute_reply":"2024-11-16T08:44:06.759805Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"tokenizer=AutoTokenizer.from_pretrained(base_model_id,padding_side='right')\nn_head=32\nn_layer=24\nn_embd=1024\nconfig_kwargs = {\"vocab_size\": len(tokenizer),\n                 \"scale_attn_by_layer_idx\": True,\n                 \"bos_token_id\":tokenizer.bos_token_id,\n                 \"eos_token_id\":tokenizer.eos_token_id,\n                 \"pad_token_id\":tokenizer.pad_token_id,\n                 \"reorder_and_upcast_attn\": True,\n                 # \"n_head\":n_head,\n                 # \"n_layer\":n_layer,\n                 # \"n_embd\":n_embd,\n                 }\n\n# Load model with config and push to hub\nconfig = AutoConfig.from_pretrained('gpt2', **config_kwargs)\nmodel = AutoModelForCausalLM.from_config(config)\nmodel = AutoModelForCausalLM.from_pretrained(base_model_id)\n# model.save_pretrained('./model')\n# model.cuda()","metadata":{"execution":{"iopub.status.busy":"2024-11-16T08:44:32.221822Z","iopub.execute_input":"2024-11-16T08:44:32.222186Z","iopub.status.idle":"2024-11-16T08:45:13.193756Z","shell.execute_reply.started":"2024-11-16T08:44:32.222151Z","shell.execute_reply":"2024-11-16T08:45:13.192920Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/46.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6b4708348e6430689bb008a4fa3b375"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a4ef5170d524105bdb54da1d600adc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c93b36b6c8347f39496d8bea95e8ef5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/555 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12507c70d74f404d80e4685afa3b411f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5895e88025fe426c9d37266527ab886e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/988 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7517d1644e3845528a6093cf16f5e02e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.13G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a3e31b2a9ca4061a13994569aeb1391"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3153fdf15c8c4fe2a345e2a453e2b6c0"}},"metadata":{}}]},{"cell_type":"code","source":"eng_text=[]\njp_text=[]\nwith open('/kaggle/input/japanese-english-subtitle-corpus/split/train','r') as f:\n    for line in f:\n        txt=(line.strip().split('\\t'))\n        eng_text.append(txt[0])\n        jp_text.append(txt[1])\ndataset={\n    'src':jp_text,\n    'trg':eng_text\n}","metadata":{"execution":{"iopub.status.busy":"2024-11-16T08:45:21.368505Z","iopub.execute_input":"2024-11-16T08:45:21.368910Z","iopub.status.idle":"2024-11-16T08:45:26.325996Z","shell.execute_reply.started":"2024-11-16T08:45:21.368874Z","shell.execute_reply":"2024-11-16T08:45:26.324997Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"raw_datasets=Dataset.from_dict(dataset).shuffle(np.random.randint(1,10000))","metadata":{"execution":{"iopub.status.busy":"2024-11-16T08:45:28.354674Z","iopub.execute_input":"2024-11-16T08:45:28.355706Z","iopub.status.idle":"2024-11-16T08:45:32.797015Z","shell.execute_reply.started":"2024-11-16T08:45:28.355664Z","shell.execute_reply":"2024-11-16T08:45:32.796126Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(raw_datasets[100]['src'],raw_datasets[100]['trg'])","metadata":{"execution":{"iopub.status.busy":"2024-11-16T08:45:32.798439Z","iopub.execute_input":"2024-11-16T08:45:32.798763Z","iopub.status.idle":"2024-11-16T08:45:32.807391Z","shell.execute_reply.started":"2024-11-16T08:45:32.798730Z","shell.execute_reply":"2024-11-16T08:45:32.806543Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"パワーを取り戻すためには...。 in order to regain one's power...\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import IterableDataset\n\nclass ConstantLengthDataset(IterableDataset):\n    def __init__(\n        self, tokenizer, dataset, infinite=False, seq_length=320, num_of_sequences=1024, chars_per_token=5.5\n    ):\n        self.tokenizer = tokenizer\n        self.tokenizer.add_eos_token=True\n        self.concat_token_id = tokenizer.bos_token_id\n        self.dataset = dataset\n        self.seq_length = seq_length\n        self.input_characters = seq_length * chars_per_token * num_of_sequences\n        self.epoch = 0\n        self.infinite = infinite\n        self.prompt=f\"Translate the following Japanese sentence to English:\\n\\nJapanese:\"\n    def __iter__(self):\n        iterator = iter(self.dataset)\n        more_examples = True\n        while more_examples:\n            buffer, buffer_len = [], 0\n            while True:\n                if buffer_len >= self.input_characters:\n                    break\n                try:\n                    cur_data=next(iterator)\n                    buffer.append(self.prompt+cur_data[\"src\"]+\"\\nEnglish:\"+cur_data[\"trg\"])\n                    buffer_len += len(buffer[-1])\n                except StopIteration:\n                    if self.infinite:\n                        iterator = iter(self.dataset)\n                        self.epoch += 1\n                        logger.info(f\"Dataset epoch: {self.epoch}\")\n                    else:\n                        more_examples = False\n                        break\n            tokenized_inputs = self.tokenizer(buffer, truncation=False)[\"input_ids\"]\n            all_token_ids = []\n            for tokenized_input in tokenized_inputs:\n                all_token_ids.extend(tokenized_input)# + [self.concat_token_id])\n            for i in range(0, len(all_token_ids), self.seq_length):\n                input_ids = all_token_ids[i : i + self.seq_length]\n                if len(input_ids) == self.seq_length:\n                    yield {'input_ids':torch.tensor(input_ids),'labels':torch.tensor(input_ids)}","metadata":{"execution":{"iopub.status.busy":"2024-11-16T08:45:37.717830Z","iopub.execute_input":"2024-11-16T08:45:37.718546Z","iopub.status.idle":"2024-11-16T08:45:37.853515Z","shell.execute_reply.started":"2024-11-16T08:45:37.718503Z","shell.execute_reply":"2024-11-16T08:45:37.852539Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_dataset = ConstantLengthDataset(\n        tokenizer, raw_datasets, infinite=True\n    )\nit=iter(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T08:45:40.239109Z","iopub.execute_input":"2024-11-16T08:45:40.239790Z","iopub.status.idle":"2024-11-16T08:45:40.244515Z","shell.execute_reply.started":"2024-11-16T08:45:40.239751Z","shell.execute_reply":"2024-11-16T08:45:40.243615Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"x=next(it)['input_ids']\nprint(len(x))\ntokenizer.decode(x)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T08:45:41.055778Z","iopub.execute_input":"2024-11-16T08:45:41.056167Z","iopub.status.idle":"2024-11-16T08:45:43.207277Z","shell.execute_reply.started":"2024-11-16T08:45:41.056131Z","shell.execute_reply":"2024-11-16T08:45:43.206410Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"320\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"\"<bos>Translate the following Japanese sentence to English:\\n\\nJapanese:これは...\\nEnglish:what the...<eos><bos>Translate the following Japanese sentence to English:\\n\\nJapanese:あくまで可能性を 言ったつもりで・・・\\nEnglish:and thanks to stacy that dream's about to come true.<eos><bos>Translate the following Japanese sentence to English:\\n\\nJapanese:そのコストがとても高いという事を述べています\\nEnglish:is very little, at a very high cost.<eos><bos>Translate the following Japanese sentence to English:\\n\\nJapanese:友喜! どうしたの? 消火器なら\\nEnglish:tomoki! what happened? if you use a fire extinguisher<eos><bos>Translate the following Japanese sentence to English:\\n\\nJapanese:5分あげます\\nEnglish:you get five minutes.<eos><bos>Translate the following Japanese sentence to English:\\n\\nJapanese:スコットにだ\\nEnglish:it's for scott.<eos><bos>Translate the following Japanese sentence to English:\\n\\nJapanese:私はガジェットに注目している1人です\\nEnglish:and i'm one of many people who believes that<eos><bos>Translate the following Japanese sentence to English:\\n\\nJapanese:シスコ 聞こえるか?\\nEnglish:cisco, can you hear me?<eos><bos>Translate the following Japanese sentence to English:\\n\\nJapanese:彼らはより円滑に作業できるでしょう\\nEnglish:they will do better this time<eos><bos>Translate the following Japanese sentence to English:\\n\\nJapanese:そう?\\nEnglish:yeah? mm.<eos><bos>Translate the following Japanese sentence to English:\\n\\nJapanese:逮捕しに行ってくるわ\\nEnglish\""},"metadata":{}}]},{"cell_type":"code","source":"# from transformers import DataCollatorForSeq2Seq\n# class TranslationDataCollator(DataCollatorForSeq2Seq):\n#     def __init__(self, tokenizer, max_length=128):\n#         self.tokenizer = tokenizer\n#         self.max_length = max_length\n\n#     def __call__(self, features):\n#         inputs = []\n#         labels = []\n        \n#         for example in features:\n#             # Get the Japanese (src) and English (trg) text\n#             src_text = example[\"src\"]\n#             trg_text = example[\"trg\"]\n\n#             # Format prompt for translation\n#             prompt = f\"Translate the following Japanese sentence to English:\\n\\nJapanese:{src_text}\"\n#             len_of_chat_template=64\n#             # Tokenize prompt and target text\n#             prompt_tokens = self.tokenizer(prompt, truncation=True, max_length=self.max_length+len_of_chat_template, add_special_tokens=False)[\"input_ids\"]\n#             trg_tokens = self.tokenizer(trg_text, truncation=True, max_length=self.max_length, add_special_tokens=False)[\"input_ids\"]\n            \n#             # Concatenate tokens with BOS and EOS\n#             input_ids = [self.tokenizer.bos_token_id] + prompt_tokens + self.tokenizer.encode(\"\\nEnglish:\",add_special_tokens=False) + trg_tokens + [self.tokenizer.eos_token_id]\n#             label_ids = input_ids.copy()  # Autoregressive model needs labels to match input\n            \n#             inputs.append(input_ids)\n#             labels.append(label_ids)\n\n#         # Pad sequences in the batch\n#         inputs = self.tokenizer.pad({\"input_ids\": inputs}, padding=True, return_tensors=\"pt\")[\"input_ids\"]\n#         labels = self.tokenizer.pad({\"input_ids\": labels}, padding=True, return_tensors=\"pt\")[\"input_ids\"]\n\n#         # print(self.tokenizer.batch_decode(inputs))\n        \n#         return {\n#             \"input_ids\": inputs,\n#             \"labels\": labels,\n#         }\n\n# # Create an instance of the data collator\n# data_collator = TranslationDataCollator(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T08:45:51.073647Z","iopub.execute_input":"2024-11-16T08:45:51.074367Z","iopub.status.idle":"2024-11-16T08:45:51.086165Z","shell.execute_reply.started":"2024-11-16T08:45:51.074327Z","shell.execute_reply":"2024-11-16T08:45:51.085131Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results\",\n#     eval_strategy=\"epoch\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=4,  # Adjust based on GPU memory\n    logging_steps=1,\n    lr_scheduler_type='cosine',\n    warmup_steps=200,\n    gradient_accumulation_steps=2,\n    num_train_epochs=1,\n    max_steps=45000,\n    weight_decay=0.01,\n    save_total_limit=1,\n    save_steps=1000,\n    push_to_hub=False,\n    remove_unused_columns=False,\n    report_to='none'\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T08:48:48.054773Z","iopub.execute_input":"2024-11-16T08:48:48.055436Z","iopub.status.idle":"2024-11-16T08:48:48.083962Z","shell.execute_reply.started":"2024-11-16T08:48:48.055397Z","shell.execute_reply":"2024-11-16T08:48:48.083134Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import neptune\nfrom transformers.integrations import NeptuneCallback\nneptune_callback = NeptuneCallback(\n    project=neptune_project,\n    api_token=neptune_api\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T08:48:49.887085Z","iopub.execute_input":"2024-11-16T08:48:49.887865Z","iopub.status.idle":"2024-11-16T08:48:49.892310Z","shell.execute_reply.started":"2024-11-16T08:48:49.887824Z","shell.execute_reply":"2024-11-16T08:48:49.891346Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,  # Raw dataset without tokenized inputs\n#     data_collator=data_collator,\n    tokenizer=tokenizer,  # Required for Seq2SeqTrainer to handle text generation and decoding,\n    callbacks=[neptune_callback]\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T08:48:50.267620Z","iopub.execute_input":"2024-11-16T08:48:50.268249Z","iopub.status.idle":"2024-11-16T08:48:50.286127Z","shell.execute_reply.started":"2024-11-16T08:48:50.268210Z","shell.execute_reply":"2024-11-16T08:48:50.285382Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-11-16T08:48:51.104436Z","iopub.execute_input":"2024-11-16T08:48:51.104945Z","iopub.status.idle":"2024-11-16T08:49:31.436956Z","shell.execute_reply.started":"2024-11-16T08:48:51.104898Z","shell.execute_reply":"2024-11-16T08:49:31.435769Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='49' max='45000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   49/45000 00:36 < 9:46:48, 1.28 it/s, Epoch 0.00/9223372036854775807]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.537900</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.624900</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3.353600</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>3.457600</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>3.362500</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>3.470600</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>3.433200</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>3.461200</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>3.440200</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>3.188400</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>3.340300</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>3.476100</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>3.405100</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>3.162000</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>3.451000</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>3.269600</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>3.367700</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>3.251600</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>3.444500</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>3.243000</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>3.109100</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>3.297400</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>3.247100</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>3.130000</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>3.088600</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>3.299400</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>3.095100</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>3.050000</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>3.098000</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>3.057300</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>2.999100</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>3.109500</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>3.098900</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>2.975800</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>3.011900</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>3.013300</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>3.133400</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>3.068500</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>3.263300</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>3.046400</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>3.025400</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>3.079400</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>3.047400</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>2.903900</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>2.972500</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>3.091700</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>2.932000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2393\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2394\u001b[0m ):\n\u001b[1;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"src_text='あなたとは遊びたくない'\nprint(tokenizer.batch_decode(model.generate(tokenizer.encode(f\"Translate the following Japanese sentence to English:\\n\\nJapanese:{src_text}\\nEnglish:\",return_tensors='pt')[:,:-1].cuda(),max_length=128))[0])","metadata":{"execution":{"iopub.status.busy":"2024-11-16T08:49:33.705200Z","iopub.execute_input":"2024-11-16T08:49:33.705602Z","iopub.status.idle":"2024-11-16T08:49:33.844142Z","shell.execute_reply.started":"2024-11-16T08:49:33.705550Z","shell.execute_reply":"2024-11-16T08:49:33.843181Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"<bos>Translate the following Japanese sentence to English:\n\nJapanese:あなたとは遊びたくない\nEnglish:i don't want to play with you.<eos>\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}