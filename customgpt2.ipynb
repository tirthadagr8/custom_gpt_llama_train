{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9908224,"sourceType":"datasetVersion","datasetId":6087585},{"sourceId":116928,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":98269,"modelId":121954}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n%pip install -U neptune","metadata":{"_uuid":"85596513-c49e-4382-b47e-c4dcdd7b45ca","_cell_guid":"6b0555db-bf27-45c8-bfa0-865cdee90692","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-14T17:57:56.919196Z","iopub.execute_input":"2024-11-14T17:57:56.919512Z","iopub.status.idle":"2024-11-14T17:58:19.548344Z","shell.execute_reply.started":"2024-11-14T17:57:56.919480Z","shell.execute_reply":"2024-11-14T17:58:19.547222Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, GPT2Config, GPT2Model, AutoConfig,\\\n        AutoModelForCausalLM, Seq2SeqTrainer, Seq2SeqTrainingArguments,Gemma2Config\nfrom datasets import load_dataset,Dataset\nfrom transformers import DataCollatorWithPadding,Trainer, TrainingArguments, DataCollatorForLanguageModeling","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T17:58:19.550700Z","iopub.execute_input":"2024-11-14T17:58:19.551331Z","iopub.status.idle":"2024-11-14T17:58:38.606625Z","shell.execute_reply.started":"2024-11-14T17:58:19.551282Z","shell.execute_reply":"2024-11-14T17:58:38.605844Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nfrom kaggle_secrets import UserSecretsClient\n\nneptune_api = UserSecretsClient().get_secret(\"NEPTUNE_API_TOKEN\")\nneptune_project =UserSecretsClient().get_secret(\"NEPTUNE_PROJECT\")\n# notebook_login(hf_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T17:58:38.607828Z","iopub.execute_input":"2024-11-14T17:58:38.608411Z","iopub.status.idle":"2024-11-14T17:58:38.968038Z","shell.execute_reply.started":"2024-11-14T17:58:38.608377Z","shell.execute_reply":"2024-11-14T17:58:38.967241Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer=AutoTokenizer.from_pretrained('/kaggle/input/gemma-2-2b-jpn-it/transformers/gemma-2-2b-jpn-it/1')\nn_head=32\nn_layer=24\nn_embd=1024\nconfig_kwargs = {\"vocab_size\": len(tokenizer),\n                 \"scale_attn_by_layer_idx\": True,\n                 \"bos_token_id\":tokenizer.bos_token_id,\n                 \"eos_token_id\":tokenizer.eos_token_id,\n                 \"reorder_and_upcast_attn\": True,\n                 # \"n_head\":n_head,\n                 # \"n_layer\":n_layer,\n                 # \"n_embd\":n_embd,\n                 }\n\n# Load model with config and push to hub\nconfig = AutoConfig.from_pretrained('gpt2', **config_kwargs)\nmodel = AutoModelForCausalLM.from_config(config)\n# model = AutoModelForCausalLM.from_pretrained('/kaggle/input/customgpt2/transformers/default/1/results/checkpoint-26512')\n# model.save_pretrained('./model')\n# model.cuda()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T17:58:38.970011Z","iopub.execute_input":"2024-11-14T17:58:38.970336Z","iopub.status.idle":"2024-11-14T17:58:54.024797Z","shell.execute_reply.started":"2024-11-14T17:58:38.970304Z","shell.execute_reply":"2024-11-14T17:58:54.023777Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eng_text=[]\njp_text=[]\nwith open('/kaggle/input/japanese-english-subtitle-corpus/split/train','r') as f:\n    for line in f:\n        txt=(line.strip().split('\\t'))\n        eng_text.append(txt[0])\n        jp_text.append(txt[1])\ndataset={\n    'src':jp_text,\n    'trg':eng_text\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T17:58:54.026130Z","iopub.execute_input":"2024-11-14T17:58:54.026504Z","iopub.status.idle":"2024-11-14T17:59:00.160178Z","shell.execute_reply.started":"2024-11-14T17:58:54.026462Z","shell.execute_reply":"2024-11-14T17:59:00.159094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"raw_datasets=Dataset.from_dict(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T17:59:00.161530Z","iopub.execute_input":"2024-11-14T17:59:00.161874Z","iopub.status.idle":"2024-11-14T17:59:03.581136Z","shell.execute_reply.started":"2024-11-14T17:59:00.161839Z","shell.execute_reply":"2024-11-14T17:59:03.580115Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\nclass TranslationDataCollator(DataCollatorForSeq2Seq):\n    def __init__(self, tokenizer, max_length=128):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __call__(self, features):\n        inputs = []\n        labels = []\n        \n        for example in features:\n            # Get the Japanese (src) and English (trg) text\n            src_text = example[\"src\"]\n            trg_text = example[\"trg\"]\n\n            # Format prompt for translation\n            prompt = f\"Translate the following Japanese sentence to English:\\n\\nJapanese: {src_text}\"\n            \n            # Tokenize prompt and target text\n            prompt_tokens = self.tokenizer(prompt, truncation=True, max_length=self.max_length, add_special_tokens=False)[\"input_ids\"]\n            trg_tokens = self.tokenizer(trg_text, truncation=True, max_length=self.max_length, add_special_tokens=False)[\"input_ids\"]\n            \n            # Concatenate tokens with BOS and EOS\n            input_ids = [self.tokenizer.bos_token_id] + prompt_tokens + self.tokenizer.encode(\"\\nEnglish:\") + trg_tokens + [self.tokenizer.eos_token_id]\n            label_ids = input_ids.copy()  # Autoregressive model needs labels to match input\n            \n            inputs.append(input_ids)\n            labels.append(label_ids)\n\n        # Pad sequences in the batch\n        inputs = self.tokenizer.pad({\"input_ids\": inputs}, padding=True, return_tensors=\"pt\")[\"input_ids\"]\n        labels = self.tokenizer.pad({\"input_ids\": labels}, padding=True, return_tensors=\"pt\")[\"input_ids\"]\n\n        return {\n            \"input_ids\": inputs,\n            \"labels\": labels,\n        }\n\n# Create an instance of the data collator\ndata_collator = TranslationDataCollator(tokenizer=tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T17:59:03.582415Z","iopub.execute_input":"2024-11-14T17:59:03.582798Z","iopub.status.idle":"2024-11-14T17:59:03.593074Z","shell.execute_reply.started":"2024-11-14T17:59:03.582752Z","shell.execute_reply":"2024-11-14T17:59:03.592017Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results\",\n#     eval_strategy=\"epoch\",\n    learning_rate=5e-4,\n    per_device_train_batch_size=16,  # Adjust based on GPU memory\n    logging_steps=1,\n    lr_scheduler_type='cosine',\n    warmup_steps=250,\n    gradient_accumulation_steps=2,\n    num_train_epochs=1,\n    max_steps=65000,\n    weight_decay=0.01,\n    save_total_limit=1,\n    save_steps=1000,\n    push_to_hub=False,\n    remove_unused_columns=False,\n    report_to='none'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T18:04:51.631893Z","iopub.execute_input":"2024-11-14T18:04:51.632309Z","iopub.status.idle":"2024-11-14T18:04:51.661255Z","shell.execute_reply.started":"2024-11-14T18:04:51.632269Z","shell.execute_reply":"2024-11-14T18:04:51.660287Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import neptune\nfrom transformers.integrations import NeptuneCallback\nneptune_callback = NeptuneCallback(\n    project=neptune_project,\n    api_token=neptune_api\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T18:04:52.509245Z","iopub.execute_input":"2024-11-14T18:04:52.509623Z","iopub.status.idle":"2024-11-14T18:04:52.514191Z","shell.execute_reply.started":"2024-11-14T18:04:52.509589Z","shell.execute_reply":"2024-11-14T18:04:52.513260Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=raw_datasets,  # Raw dataset without tokenized inputs\n    data_collator=data_collator,\n    tokenizer=tokenizer,  # Required for Seq2SeqTrainer to handle text generation and decoding,\n    callbacks=[neptune_callback]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T18:04:53.041621Z","iopub.execute_input":"2024-11-14T18:04:53.042385Z","iopub.status.idle":"2024-11-14T18:04:53.060364Z","shell.execute_reply.started":"2024-11-14T18:04:53.042347Z","shell.execute_reply":"2024-11-14T18:04:53.059579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T18:04:53.609236Z","iopub.execute_input":"2024-11-14T18:04:53.610133Z","iopub.status.idle":"2024-11-14T18:05:39.800729Z","shell.execute_reply.started":"2024-11-14T18:04:53.610096Z","shell.execute_reply":"2024-11-14T18:05:39.799438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"src_text='素人の気づき 「いい、アーニャ。今から行ったとしても、陛下が実際'\nprint(tokenizer.batch_decode(model.generate(tokenizer.encode(f\"Translate the following Japanese sentence to English:\\n\\nJapanese: {src_text}\\nEnglish:\",return_tensors='pt').cuda(),max_length=128))[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T18:06:07.833150Z","iopub.execute_input":"2024-11-14T18:06:07.833933Z","iopub.status.idle":"2024-11-14T18:06:08.015916Z","shell.execute_reply.started":"2024-11-14T18:06:07.833893Z","shell.execute_reply":"2024-11-14T18:06:08.014915Z"}},"outputs":[],"execution_count":null}]}