{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9920640,"sourceType":"datasetVersion","datasetId":6097009},{"sourceId":167593,"sourceType":"modelInstanceVersion","modelInstanceId":142577,"modelId":165157}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n%pip install -U neptune\n# %pip install -U transformers","metadata":{"_uuid":"85596513-c49e-4382-b47e-c4dcdd7b45ca","_cell_guid":"6b0555db-bf27-45c8-bfa0-865cdee90692","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-16T06:19:33.740869Z","iopub.execute_input":"2024-11-16T06:19:33.741760Z","iopub.status.idle":"2024-11-16T06:19:45.558843Z","shell.execute_reply.started":"2024-11-16T06:19:33.741709Z","shell.execute_reply":"2024-11-16T06:19:45.557622Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, GPT2Config, GPT2Model, AutoConfig,\\\n        AutoModelForCausalLM, Seq2SeqTrainer, Seq2SeqTrainingArguments,Gemma2Config\nfrom datasets import load_dataset,Dataset\nimport numpy as np\nimport torch\nfrom transformers import DataCollatorWithPadding,Trainer, TrainingArguments, DataCollatorForLanguageModeling","metadata":{"execution":{"iopub.status.busy":"2024-11-16T06:19:45.564620Z","iopub.execute_input":"2024-11-16T06:19:45.564987Z","iopub.status.idle":"2024-11-16T06:19:52.789306Z","shell.execute_reply.started":"2024-11-16T06:19:45.564943Z","shell.execute_reply":"2024-11-16T06:19:52.788340Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nfrom kaggle_secrets import UserSecretsClient\n\nneptune_api = UserSecretsClient().get_secret(\"NEPTUNE_API_TOKEN\")\nneptune_project =UserSecretsClient().get_secret(\"NEPTUNE_PROJECT\")\n# notebook_login(hf_token)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T06:19:52.790582Z","iopub.execute_input":"2024-11-16T06:19:52.791161Z","iopub.status.idle":"2024-11-16T06:19:52.921552Z","shell.execute_reply.started":"2024-11-16T06:19:52.791125Z","shell.execute_reply":"2024-11-16T06:19:52.920734Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"tokenizer=AutoTokenizer.from_pretrained('/kaggle/input/customgpt2/transformers/default/1/results/checkpoint-80000',padding_side='right')\nn_head=32\nn_layer=24\nn_embd=1024\nconfig_kwargs = {\"vocab_size\": len(tokenizer),\n                 \"scale_attn_by_layer_idx\": True,\n                 \"bos_token_id\":tokenizer.bos_token_id,\n                 \"eos_token_id\":tokenizer.eos_token_id,\n                 \"pad_token_id\":tokenizer.pad_token_id,\n                 \"reorder_and_upcast_attn\": True,\n                 # \"n_head\":n_head,\n                 # \"n_layer\":n_layer,\n                 # \"n_embd\":n_embd,\n                 }\n\n# Load model with config and push to hub\nconfig = AutoConfig.from_pretrained('gpt2', **config_kwargs)\nmodel = AutoModelForCausalLM.from_config(config)\nmodel = AutoModelForCausalLM.from_pretrained('/kaggle/input/customgpt2/transformers/default/1/results/checkpoint-80000')\n# model.save_pretrained('./model')\n# model.cuda()","metadata":{"execution":{"iopub.status.busy":"2024-11-16T06:19:52.923998Z","iopub.execute_input":"2024-11-16T06:19:52.924326Z","iopub.status.idle":"2024-11-16T06:20:02.938631Z","shell.execute_reply.started":"2024-11-16T06:19:52.924292Z","shell.execute_reply":"2024-11-16T06:20:02.937577Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"eng_text=[]\njp_text=[]\nwith open('/kaggle/input/japanese-english-subtitle-corpus/split/train','r') as f:\n    for line in f:\n        txt=(line.strip().split('\\t'))\n        eng_text.append(txt[0])\n        jp_text.append(txt[1])\ndataset={\n    'src':jp_text,\n    'trg':eng_text\n}","metadata":{"execution":{"iopub.status.busy":"2024-11-16T06:20:02.942410Z","iopub.execute_input":"2024-11-16T06:20:02.943112Z","iopub.status.idle":"2024-11-16T06:20:07.229482Z","shell.execute_reply.started":"2024-11-16T06:20:02.943070Z","shell.execute_reply":"2024-11-16T06:20:07.228410Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"raw_datasets=Dataset.from_dict(dataset).shuffle(np.random.randint(1,10000))","metadata":{"execution":{"iopub.status.busy":"2024-11-16T06:20:07.231026Z","iopub.execute_input":"2024-11-16T06:20:07.231578Z","iopub.status.idle":"2024-11-16T06:20:11.413151Z","shell.execute_reply.started":"2024-11-16T06:20:07.231540Z","shell.execute_reply":"2024-11-16T06:20:11.412111Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(raw_datasets[100]['src'],raw_datasets[100]['trg'])","metadata":{"execution":{"iopub.status.busy":"2024-11-16T06:20:11.414586Z","iopub.execute_input":"2024-11-16T06:20:11.415045Z","iopub.status.idle":"2024-11-16T06:20:11.421626Z","shell.execute_reply.started":"2024-11-16T06:20:11.414993Z","shell.execute_reply":"2024-11-16T06:20:11.420554Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"確かに過去の備えでは無かったが 我々は武器を持っている now, this city may not have the manpower it once did... but it has the firepower.\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import IterableDataset\n\nclass ConstantLengthDataset(IterableDataset):\n    def __init__(\n        self, tokenizer, dataset, infinite=False, seq_length=320, num_of_sequences=1024, chars_per_token=5.5\n    ):\n        self.tokenizer = tokenizer\n        self.tokenizer.add_eos_token=True\n        self.concat_token_id = tokenizer.bos_token_id\n        self.dataset = dataset\n        self.seq_length = seq_length\n        self.input_characters = seq_length * chars_per_token * num_of_sequences\n        self.epoch = 0\n        self.infinite = infinite\n        self.prompt=f\"Translate the following Japanese sentence to English:\\n\\nJapanese:\"\n    def __iter__(self):\n        iterator = iter(self.dataset)\n        more_examples = True\n        while more_examples:\n            buffer, buffer_len = [], 0\n            while True:\n                if buffer_len >= self.input_characters:\n                    break\n                try:\n                    cur_data=next(iterator)\n                    buffer.append(self.prompt+cur_data[\"src\"]+\"\\nEnglish:\"+cur_data[\"trg\"])\n                    buffer_len += len(buffer[-1])\n                except StopIteration:\n                    if self.infinite:\n                        iterator = iter(self.dataset)\n                        self.epoch += 1\n                        logger.info(f\"Dataset epoch: {self.epoch}\")\n                    else:\n                        more_examples = False\n                        break\n            tokenized_inputs = self.tokenizer(buffer, truncation=False)[\"input_ids\"]\n            all_token_ids = []\n            for tokenized_input in tokenized_inputs:\n                all_token_ids.extend(tokenized_input)# + [self.concat_token_id])\n            for i in range(0, len(all_token_ids), self.seq_length):\n                input_ids = all_token_ids[i : i + self.seq_length]\n                if len(input_ids) == self.seq_length:\n                    yield {'input_ids':torch.tensor(input_ids),'labels':torch.tensor(input_ids)}","metadata":{"execution":{"iopub.status.busy":"2024-11-16T06:20:11.423362Z","iopub.execute_input":"2024-11-16T06:20:11.424070Z","iopub.status.idle":"2024-11-16T06:20:11.437475Z","shell.execute_reply.started":"2024-11-16T06:20:11.424023Z","shell.execute_reply":"2024-11-16T06:20:11.436636Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_dataset = ConstantLengthDataset(\n        tokenizer, raw_datasets, infinite=True\n    )\nit=iter(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T06:20:11.438525Z","iopub.execute_input":"2024-11-16T06:20:11.438837Z","iopub.status.idle":"2024-11-16T06:20:11.449972Z","shell.execute_reply.started":"2024-11-16T06:20:11.438806Z","shell.execute_reply":"2024-11-16T06:20:11.449170Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"x=next(it)['input_ids']\nprint(len(x))\ntokenizer.decode(x)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T06:20:11.451014Z","iopub.execute_input":"2024-11-16T06:20:11.451350Z","iopub.status.idle":"2024-11-16T06:20:13.256264Z","shell.execute_reply.started":"2024-11-16T06:20:11.451305Z","shell.execute_reply":"2024-11-16T06:20:13.255376Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"320\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"\"<bos>Translate the following Japanese sentence to English:\\n\\nJapanese:また ヒゲの部分には\\nEnglish:it's also got a little chin barbel here<eos><bos>Translate the following Japanese sentence to English:\\n\\nJapanese:新鮮な空気を\\nEnglish:sme fresh air.<eos><bos>Translate the following Japanese sentence to English:\\n\\nJapanese:ルーカス・デサンジュだ\\nEnglish:i'm family. lucas desange.<eos><bos>Translate the following Japanese sentence to English:\\n\\nJapanese:性的暴行は?\\nEnglish:any sexual violence?<eos><bos>Translate the following Japanese sentence to English:\\n\\nJapanese:しもた! 何やっとると?\\nEnglish:what are you doing? i'm coming in!<eos><bos>Translate the following Japanese sentence to English:\\n\\nJapanese:スイーツがあなたの昇進前の 最後の事件になるって言ってた すごいところを見せてよ\\nEnglish:sweets would say that since this may be your last case before being promoted, you need to prove that you're still the best.<eos><bos>Translate the following Japanese sentence to English:\\n\\nJapanese:e・ハーモニーのトッド・マハーです ご用件は?\\nEnglish:hi! todd mahar, eharmony. how can i help you today?<eos><bos>Translate the following Japanese sentence to English:\\n\\nJapanese:キングコブラは いないよな?\\nEnglish:no king cobras, right?<eos><bos>Translate the following Japanese sentence to English:\\n\\nJapanese:何百本も見たわ\\nEnglish:i looked at hundreds of 'em.<eos><bos>Translate the\""},"metadata":{}}]},{"cell_type":"code","source":"# from transformers import DataCollatorForSeq2Seq\n# class TranslationDataCollator(DataCollatorForSeq2Seq):\n#     def __init__(self, tokenizer, max_length=128):\n#         self.tokenizer = tokenizer\n#         self.max_length = max_length\n\n#     def __call__(self, features):\n#         inputs = []\n#         labels = []\n        \n#         for example in features:\n#             # Get the Japanese (src) and English (trg) text\n#             src_text = example[\"src\"]\n#             trg_text = example[\"trg\"]\n\n#             # Format prompt for translation\n#             prompt = f\"Translate the following Japanese sentence to English:\\n\\nJapanese:{src_text}\"\n#             len_of_chat_template=64\n#             # Tokenize prompt and target text\n#             prompt_tokens = self.tokenizer(prompt, truncation=True, max_length=self.max_length+len_of_chat_template, add_special_tokens=False)[\"input_ids\"]\n#             trg_tokens = self.tokenizer(trg_text, truncation=True, max_length=self.max_length, add_special_tokens=False)[\"input_ids\"]\n            \n#             # Concatenate tokens with BOS and EOS\n#             input_ids = [self.tokenizer.bos_token_id] + prompt_tokens + self.tokenizer.encode(\"\\nEnglish:\",add_special_tokens=False) + trg_tokens + [self.tokenizer.eos_token_id]\n#             label_ids = input_ids.copy()  # Autoregressive model needs labels to match input\n            \n#             inputs.append(input_ids)\n#             labels.append(label_ids)\n\n#         # Pad sequences in the batch\n#         inputs = self.tokenizer.pad({\"input_ids\": inputs}, padding=True, return_tensors=\"pt\")[\"input_ids\"]\n#         labels = self.tokenizer.pad({\"input_ids\": labels}, padding=True, return_tensors=\"pt\")[\"input_ids\"]\n\n#         # print(self.tokenizer.batch_decode(inputs))\n        \n#         return {\n#             \"input_ids\": inputs,\n#             \"labels\": labels,\n#         }\n\n# # Create an instance of the data collator\n# data_collator = TranslationDataCollator(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T06:20:13.257400Z","iopub.execute_input":"2024-11-16T06:20:13.257713Z","iopub.status.idle":"2024-11-16T06:20:13.263871Z","shell.execute_reply.started":"2024-11-16T06:20:13.257679Z","shell.execute_reply":"2024-11-16T06:20:13.262902Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results\",\n#     eval_strategy=\"epoch\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=4,  # Adjust based on GPU memory\n    logging_steps=20,\n    lr_scheduler_type='cosine',\n    warmup_steps=200,\n    gradient_accumulation_steps=2,\n    num_train_epochs=1,\n    max_steps=45000,\n    weight_decay=0.01,\n    save_total_limit=1,\n    save_steps=1000,\n    push_to_hub=False,\n    remove_unused_columns=False,\n    report_to='none'\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T06:25:28.359583Z","iopub.execute_input":"2024-11-16T06:25:28.360611Z","iopub.status.idle":"2024-11-16T06:25:28.389056Z","shell.execute_reply.started":"2024-11-16T06:25:28.360564Z","shell.execute_reply":"2024-11-16T06:25:28.388201Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"import neptune\nfrom transformers.integrations import NeptuneCallback\nneptune_callback = NeptuneCallback(\n    project=neptune_project,\n    api_token=neptune_api\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T06:25:29.348854Z","iopub.execute_input":"2024-11-16T06:25:29.349239Z","iopub.status.idle":"2024-11-16T06:25:29.354232Z","shell.execute_reply.started":"2024-11-16T06:25:29.349204Z","shell.execute_reply":"2024-11-16T06:25:29.353217Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,  # Raw dataset without tokenized inputs\n#     data_collator=data_collator,\n    tokenizer=tokenizer,  # Required for Seq2SeqTrainer to handle text generation and decoding,\n    callbacks=[neptune_callback]\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T06:25:29.748971Z","iopub.execute_input":"2024-11-16T06:25:29.749965Z","iopub.status.idle":"2024-11-16T06:25:29.765892Z","shell.execute_reply.started":"2024-11-16T06:25:29.749920Z","shell.execute_reply":"2024-11-16T06:25:29.764980Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-11-16T06:25:30.758778Z","iopub.execute_input":"2024-11-16T06:25:30.759571Z","iopub.status.idle":"2024-11-16T06:33:35.236437Z","shell.execute_reply.started":"2024-11-16T06:25:30.759531Z","shell.execute_reply":"2024-11-16T06:33:35.234925Z"},"trusted":true},"execution_count":37,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='612' max='45000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  612/45000 08:00 < 9:42:31, 1.27 it/s, Epoch 0.01/9223372036854775807]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>2.444500</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.434300</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.427600</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>2.381400</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.319600</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>2.568200</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>2.546000</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>2.566200</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>2.518000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.532900</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>2.535300</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>2.542500</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>2.513300</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>2.547700</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>2.507100</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>2.484300</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>2.518200</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>2.495300</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>2.525000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>2.518400</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>2.498500</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>2.487300</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>2.484800</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>2.489500</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>2.494600</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>2.531100</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>2.494000</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>2.489600</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>2.479400</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>2.480500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2394\u001b[0m ):\n\u001b[1;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3518\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3516\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3518\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2196\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2196\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"src_text='ないんたけとな。吠えるな、園山カンナっていや、'\nprint(tokenizer.batch_decode(model.generate(tokenizer.encode(f\"Translate the following Japanese sentence to English:\\n\\nJapanese:{src_text}\\nEnglish:\",return_tensors='pt')[:,:-1].cuda(),max_length=128))[0])","metadata":{"execution":{"iopub.status.busy":"2024-11-16T06:42:21.419282Z","iopub.execute_input":"2024-11-16T06:42:21.419685Z","iopub.status.idle":"2024-11-16T06:42:21.711511Z","shell.execute_reply.started":"2024-11-16T06:42:21.419651Z","shell.execute_reply":"2024-11-16T06:42:21.710389Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"<bos>Translate the following Japanese sentence to English:\n\nJapanese:ないんたけとな。吠えるな、園山カンナっていや、\nEnglish:no, no, no, no, no, no, no, no, no, no, no, no.<eos>\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}